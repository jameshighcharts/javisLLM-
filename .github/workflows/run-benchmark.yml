name: Run LLM Benchmark
on:
  workflow_dispatch:
    inputs:
      trigger_id:
        description: "UI trigger id"
        required: false
        default: ""
      our_terms:
        description: "Brand terms (comma-separated)"
        required: false
        default: "Highcharts"
      model:
        description: "OpenAI model"
        required: false
        default: "gpt-4o-mini"
      runs:
        description: "Runs per prompt (1-3)"
        required: false
        default: "3"
      temperature:
        description: "Sampling temperature (0-2)"
        required: false
        default: "0.7"
      web_search:
        description: "Enable web search"
        required: false
        default: "true"
      run_month:
        description: "Optional override (YYYY-MM)"
        required: false
        default: ""

permissions:
  contents: read

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    env:
      PYTHONUNBUFFERED: "1"
    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4.3.1

      - name: Setup Python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5.6.0
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -r requirements.txt

      - name: Build run metadata
        id: meta
        env:
          RUN_MONTH_INPUT: ${{ github.event.inputs.run_month }}
        run: |
          set -euo pipefail
          if [[ -n "${RUN_MONTH_INPUT}" ]]; then
            RUN_MONTH="${RUN_MONTH_INPUT}"
          else
            RUN_MONTH="$(date +%Y-%m)"
          fi
          RUN_ID="$(python -c 'import uuid; print(uuid.uuid4())')"
          echo "run_month=${RUN_MONTH}" >> "$GITHUB_OUTPUT"
          echo "run_id=${RUN_ID}" >> "$GITHUB_OUTPUT"
          echo "RUN_MONTH=${RUN_MONTH}" >> "$GITHUB_ENV"
          echo "RUN_ID=${RUN_ID}" >> "$GITHUB_ENV"

      - name: Validate workflow inputs
        env:
          OUR_TERMS_INPUT: ${{ github.event.inputs.our_terms }}
          MODEL_INPUT: ${{ github.event.inputs.model }}
          RUNS_INPUT: ${{ github.event.inputs.runs }}
          TEMP_INPUT: ${{ github.event.inputs.temperature }}
          WEB_SEARCH_INPUT: ${{ github.event.inputs.web_search }}
          RUN_MONTH_INPUT: ${{ github.event.inputs.run_month }}
        run: |
          set -euo pipefail

          if [[ ! "${RUNS_INPUT}" =~ ^[0-9]+$ ]]; then
            echo "runs must be an integer"
            exit 1
          fi
          if (( RUNS_INPUT < 1 || RUNS_INPUT > 3 )); then
            echo "runs must be between 1 and 3"
            exit 1
          fi

          if ! [[ "${TEMP_INPUT}" =~ ^([0-1](\.[0-9]+)?|2(\.0+)?)$ ]]; then
            echo "temperature must be a number between 0 and 2"
            exit 1
          fi

          if [[ ! "${MODEL_INPUT}" =~ ^[A-Za-z0-9_.:-]{1,64}$ ]]; then
            echo "model contains unsupported characters or is too long"
            exit 1
          fi

          if (( ${#OUR_TERMS_INPUT} > 300 )); then
            echo "our_terms is too long (max 300 chars)"
            exit 1
          fi

          if [[ "${WEB_SEARCH_INPUT}" != "true" && "${WEB_SEARCH_INPUT}" != "false" ]]; then
            echo "web_search must be true or false"
            exit 1
          fi

          if [[ -n "${RUN_MONTH_INPUT}" ]] && [[ ! "${RUN_MONTH_INPUT}" =~ ^[0-9]{4}-(0[1-9]|1[0-2])$ ]]; then
            echo "run_month must use YYYY-MM"
            exit 1
          fi

      - name: Pull live config from Supabase
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          set -euo pipefail
          python scripts/pull_config_from_supabase.py \
            --output config/benchmark_config.json

      - name: Run prompt benchmark
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          RUNS_INPUT: ${{ github.event.inputs.runs }}
          TEMP_INPUT: ${{ github.event.inputs.temperature }}
          OUR_TERMS_INPUT: ${{ github.event.inputs.our_terms }}
          MODEL_INPUT: ${{ github.event.inputs.model }}
          WEB_SEARCH_INPUT: ${{ github.event.inputs.web_search }}
        run: |
          set -euo pipefail
          CMD=(
            python llm_mention_benchmark.py
            --our-terms "${OUR_TERMS_INPUT}"
            --model "${MODEL_INPUT}"
            --runs "${RUNS_INPUT}"
            --temperature "${TEMP_INPUT}"
            --output-dir output
            --config config/benchmark_config.json
          )
          if [[ "${WEB_SEARCH_INPUT}" == "true" ]]; then
            CMD+=(--web-search)
          fi
          "${CMD[@]}"

      - name: Build scoring datasets
        run: |
          set -euo pipefail
          python scripts/build_looker_dataset.py \
            --output-dir output \
            --run-month "${RUN_MONTH}" \
            --run-id "${RUN_ID}"

      - name: Sync config + run data to Supabase
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          set -euo pipefail
          python scripts/push_to_supabase.py \
            --config config/benchmark_config.json \
            --output-dir output \
            --run-month "${RUN_MONTH}" \
            --run-id "${RUN_ID}"

      - name: Upload output artifacts
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: benchmark-${{ env.RUN_MONTH }}-${{ env.RUN_ID }}
          path: |
            output/llm_outputs.jsonl
            output/comparison_table.csv
            output/viability_index.csv
            output/looker_studio_table_paste.csv
            output/looker_kpi.csv
            output/looker_competitor_chart.csv
          if-no-files-found: warn
